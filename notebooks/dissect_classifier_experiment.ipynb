{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Dissection (for classifiers)\n",
    "\n",
    "In this notebook, we will examine internal layer representations for a classifier trained to recognize scene categories.\n",
    "\n",
    "Setup matplotlib, torch, and numpy for a high-resolution browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from importlib import reload\n",
    "import IPython\n",
    "mpl.rcParams['lines.linewidth'] = 0.25\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.linewidth'] = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up experiment directory and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, argparse, os, shutil, inspect, json, numpy, math\n",
    "import netdissect\n",
    "from netdissect.easydict import EasyDict\n",
    "from netdissect import pbar, nethook, renormalize, parallelfolder, pidfile\n",
    "from netdissect import upsample, tally, imgviz, imgsave, bargraph, show\n",
    "from experiment import dissect_experiment\n",
    "\n",
    "model_name='DA-SS-sketch'\n",
    "# choices are alexnet, vgg16, or resnet152.\n",
    "args = EasyDict(model=model_name, dataset='pacs-c', seg='netpqc', layer='layer0', quantile=0.01)\n",
    "resdir = 'results/%s-%s-%s-%s-%s' % (args.model, args.dataset, args.seg, args.layer, int(args.quantile * 1000))\n",
    "print(resdir)\n",
    "def resfile(f):\n",
    "    return os.path.join(resdir, f)\n",
    "\n",
    "\n",
    "torch.cuda.set_device(6)\n",
    "\n",
    "model = torch.load('./'+model_name+'-t001-torch16-0.pkl',map_location='cpu')\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dissect_experiment.load_dataset(args)\n",
    "sample_size = len(dataset)\n",
    "\n",
    "print(len(dataset))\n",
    "print(dataset[0][0].shape)\n",
    "print(dataset[2][1])\n",
    "\n",
    "# Classifier labels\n",
    "from urllib.request import urlopen\n",
    "from netdissect import renormalize\n",
    "\n",
    "classlabels = dataset.classes\n",
    "print(classlabels)\n",
    "renorm = renormalize.renormalizer(dataset, target='zc')\n",
    "\n",
    "# \u7531\u4e8e base \u73af\u5883\u914d\u7f6e\u95ee\u9898\uff0c\u6682\u65f6\u65e0\u6cd5\u52a0\u8f7d Segmentation \u6a21\u578b\n",
    "# segmodel, seglabels, segcatlabels = experiment.setting.load_segmenter(args.seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load classifier model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(6)\n",
    "\n",
    "# model = experiment.load_model(args)\n",
    "# layername = experiment.instrumented_layername(args)\n",
    "# model.retain_layer(layername)\n",
    "# dataset = experiment.load_dataset(args)\n",
    "# upfn = experiment.make_upfn(args, dataset, model, layername)\n",
    "# sample_size = len(dataset)\n",
    "# percent_level = 1.0 - args.quantile\n",
    "\n",
    "# print('Inspecting layer %s of model %s on %s' % (layername, args.model, args.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load segmenter, segment labels, classifier labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test classifier on some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import renormalize\n",
    "\n",
    "indices = [624]\n",
    "layername = 'layer2'\n",
    "\n",
    "# None,... \u7684\u7528\u6cd5\u662f\u6539\u53d8\u6570\u636e\u7684\u7eac\u5ea6\uff0c\u5728\u6307\u5b9a\u4f4d\u7f6e\u589e\u52a0\u4e00\u7ef4\n",
    "# batch.shape: B*Channel*H*W\n",
    "\n",
    "batch = torch.cat([dataset[i][0][None,...] for i in indices])\n",
    "truth = [classlabels[dataset[i][1]] for i in indices]\n",
    "#res.shape=B*C => 8 * 365(No. of classes)\n",
    "activations, res = model(batch.cuda())\n",
    "preds = res.max(1)[1]\n",
    "imgs = [renormalize.as_image(t, source=dataset) for t in batch]\n",
    "prednames = [classlabels[p.item()] for p in preds]\n",
    "\n",
    "show([[img, 'pred: ' + pred, 'true: ' + gt] for img, pred, gt in zip(imgs, prednames, truth)])\n",
    "\n",
    "from netdissect import imgviz\n",
    "acts = activations[layername].detach().cpu()\n",
    "\n",
    "test = acts[0][18].view(1, -1)\n",
    "print((test == 0).sum(), test.shape)\n",
    "print(acts[0][18].sum())\n",
    "\n",
    "ivsmall = imgviz.ImageVisualizer((100, 100), source=dataset)\n",
    "# u \u662f\u901a\u9053\u7f16\u53f7\uff0c\u5bf9\u4e8e conv5_3 \u6765\u8bf4\u5c31\u662f min(512, 12)\n",
    "display(show.blocks(\n",
    "    [[[ivsmall.masked_image(batch[0], acts, (0, u), percent_level=0.10)],\n",
    "      [ivsmall.heatmap(acts, (0, u), mode='nearest')]] for u in [6, 20, 73, 91]]\n",
    "))\n",
    "\n",
    "num_units = acts.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "segment single image, and visualize the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from netdissect import imgviz\n",
    "\n",
    "# iv = imgviz.ImageVisualizer(120, source=dataset)\n",
    "# seg = segmodel.segment_batch(renorm(batch).cuda(), downsample=4)\n",
    "\n",
    "# show([(iv.image(batch[i]), iv.segmentation(seg[i,0]),\n",
    "#             iv.segment_key(seg[i,-1], segmodel))\n",
    "#             for i in range(len(seg))])\n",
    "print(activations.keys())\n",
    "print(activations['max_pool'].sum())\n",
    "\n",
    "upfn = dissect_experiment.make_upfn_without_hooks(args, dataset, layername, activations)\n",
    "percent_level = 1.0 - args.quantile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize activations for single layer of single image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect quantile statistics\n",
    "\n",
    "First, unconditional quantiles over the activations.  We will upsample them to 56x56 to match with segmentations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.descnext('rq')\n",
    "def compute_samples(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    activations, _ = model(image_batch)\n",
    "    acts = activations[layername].detach()\n",
    "    hacts = upfn(acts)\n",
    "    return hacts.permute(0, 2, 3, 1).contiguous().view(-1, acts.shape[1])\n",
    "rq = tally.tally_quantile(compute_samples, dataset,\n",
    "                          sample_size=sample_size,\n",
    "                          r=8192,\n",
    "                          num_workers=100,\n",
    "                          pin_memory=True,\n",
    "                          cachefile=resfile('rq.npz'))\n",
    "\n",
    "\n",
    "pbar.descnext('topk')\n",
    "def compute_image_max(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    activations, _ = model(image_batch)\n",
    "    acts = activations[layername].detach()\n",
    "    acts = acts.view(acts.shape[0], acts.shape[1], -1)\n",
    "    acts = acts.max(2)[0]\n",
    "    return acts\n",
    "\n",
    "#topk => (64X100, 64X100)\n",
    "topk = tally.tally_topk(compute_image_max, dataset, sample_size=sample_size,\n",
    "        batch_size=50, num_workers=30, pin_memory=True,\n",
    "        cachefile=resfile('topk.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single image visualization\n",
    "rank=0\n",
    "percent_level=0.10\n",
    "unit_number = 73\n",
    "layername='layer2'\n",
    "\n",
    "print(topk.result()[0][unit_number][rank].item(), \n",
    "      topk.result()[1][unit_number][rank].item(), \n",
    "      dataset.images[topk.result()[1][unit_number][rank]])\n",
    "\n",
    "image_number = topk.result()[1][unit_number][rank].item()\n",
    "# image_number=624\n",
    "# unit_number=18\n",
    "\n",
    "iv = imgviz.ImageVisualizer((224, 224), source=dataset, quantiles=rq,\n",
    "        level=rq.quantiles(percent_level))\n",
    "batch = torch.cat([dataset[i][0][None,...] for i in [image_number]])\n",
    "truth = [classlabels[dataset[i][1]] for i in [image_number]]\n",
    "\n",
    "activations, output = model(batch.cuda())\n",
    "\n",
    "preds = output.max(1)[1]\n",
    "acts = activations[layername].detach()\n",
    "\n",
    "imgs = [renormalize.as_image(t, source=dataset) for t in batch]\n",
    "prednames = [classlabels[p.item()] for p in preds]\n",
    "\n",
    "show([[img, 'pred: ' + pred, 'true: ' + gt] for img, pred, gt in zip(imgs, prednames, truth)])\n",
    "show([[iv.masked_image(batch[0], acts, (0, unit_number))]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topk.result()[0].shape)\n",
    "print(topk.result()[1].shape)\n",
    "print(topk.result()[0][18][0], topk.result()[1][18][0])\n",
    "print(topk.result()[0][18][1], topk.result()[1][18][1])\n",
    "print(topk.result()[0][18][2], topk.result()[1][18][2])\n",
    "print(topk.result()[0][10][99], topk.result()[1][10][99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just need to run through and visualize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar.descnext('unit_images')\n",
    "\n",
    "iv = imgviz.ImageVisualizer((100, 100), source=dataset, quantiles=rq,\n",
    "        level=rq.quantiles(percent_level))\n",
    "def compute_acts(*image_batch):\n",
    "    image_batch = image_batch[0].cuda()\n",
    "    activations, _ = model(image_batch)\n",
    "    acts_batch = activations[layername]\n",
    "    return acts_batch\n",
    "unit_images = iv.masked_images_for_topk(\n",
    "        compute_acts, dataset, topk, k=5, num_workers=30, pin_memory=True,\n",
    "        cachefile=resfile('top5images.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in [17, 18, 36, 38, 48]:\n",
    "    print( 'unit %d' % u)\n",
    "    display(unit_images[u])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Units\n",
    "\n",
    "Collect 99% quantile stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_at_99 = rq.quantiles(percent_level).cuda()[None,:,None,None]\n",
    "# Use the segmodel for segmentations.  With broden, we could use ground truth instead.\n",
    "def compute_conditional_indicator(batch, *args):\n",
    "    image_batch = batch.cuda()\n",
    "    seg = segmodel.segment_batch(renorm(image_batch), downsample=4)\n",
    "    _ = model(image_batch)\n",
    "    acts = model.retained_layer(layername)\n",
    "    hacts = upfn(acts)\n",
    "    iacts = (hacts > level_at_99).float() # indicator\n",
    "    return tally.conditional_samples(iacts, seg)\n",
    "pbar.descnext('condi99')\n",
    "condi99 = tally.tally_conditional_mean(compute_conditional_indicator,\n",
    "        dataset, sample_size=sample_size,\n",
    "        num_workers=3, pin_memory=True,\n",
    "        cachefile=resfile('condi99.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_99 = tally.iou_from_conditional_indicator_mean(condi99)\n",
    "unit_label_99 = [\n",
    "        (concept.item(), seglabels[concept], segcatlabels[concept], bestiou.item())\n",
    "        for (bestiou, concept) in zip(*iou_99.max(0))]\n",
    "label_list = [labelcat for concept, label, labelcat, iou in unit_label_99 if iou > 0.04]\n",
    "display(IPython.display.SVG(experiment.graph_conceptcatlist(label_list)))\n",
    "len(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a few units with their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in [10, 20, 30, 40]:\n",
    "    print('unit %d, label %s, iou %.3f' % (u, unit_label_99[u][1], unit_label_99[u][3]))\n",
    "    display(unit_images[u])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}